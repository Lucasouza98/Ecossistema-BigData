Sqoop - Importação BD Sakila – Carga Incremental

Realizar com uso do MySQL

1. Criar a tabela cp_rental_append, contendo a cópia da tabela rental com os campos rental_id e rental_date
create table cp_rental_append as select rental_id, rental_date from rental;


2 .Criar a tabela cp_rental_id e cp_rental_date, contendo a cópia da tabela cp_rental_append
create table cp_rental_id as select * from cp_rental_append;
create table cp_rental_date as select * from cp_rental_append;

 

Realizar com uso do Sqoop - Importações no warehouse /user/hive/warehouse/db_test3 e visualizar no HDFS

3. Importar as tabelas cp_rental_append, cp_rental_id e cp_rental_date com 1 mapeador
sqoop import --table cp_rental_append --connect jdbc:mysql://database/sakila --username=root --password=secret --warehouse-dir /user/hive/warehouse/db_test3 -m 1
sqoop import --table cp_rental_id --connect jdbc:mysql://database/sakila --username=root --password=secret --warehouse-dir /user/hive/warehouse/db_test3 -m 1
sqoop import --table cp_rental_date --connect jdbc:mysql://database/sakila --username=root --password=secret --warehouse-dir /user/hive/warehouse/db_test3 -m 1

hadoop fs -ls /user/hive/warehouse/db_test3
hadoop fs -ls /user/hive/warehouse/db_test3/cp_rental_append
hadoop fs -ls /user/hive/warehouse/db_test3/cp_rental_date
hadoop fs -ls /user/hive/warehouse/db_test3/cp_rental_id

/user/hive/warehouse/db_test3/cp_rental_append/part-m-00000 -> VISUALIZAR CONTEUDO NO ARQUIVO
/user/hive/warehouse/db_test3/cp_rental_date/part-m-00000 -> VISUALIZAR CONTEUDO NO ARQUIVO
/user/hive/warehouse/db_test3/cp_rental_id/part-m-00000 -> VISUALIZAR CONTEUDO NO ARQUIVO
 

Realizar com uso do MySQL

4. Executar o sql /db-sql/sakila/insert_rental.sql no container do database

$ docker exec -it database bash

$ cd /db-sql/sakila

$ mysql -psecret < insert_rental.sql

ultimo valor: 16049       | 2005-08-23 22:50:12.0
 

Realizar com uso do Sqoop - Importações no warehouse /user/hive/warehouse/db_test3 e visualizar no HDFS

5. Atualizar a tabela cp_rental_append no HDFS anexando os novos arquivos
sqoop import --table cp_rental_append --connect jdbc:mysql://database/sakila --username=root --password=secret --warehouse-dir /user/hive/warehouse/db_test3 -m 1 --append

6. Atualizar a tabela cp_rental_id no HDFS de acordo com o último registro de rental_id, adicionando apenas os novos dados.
sqoop eval --connect jdbc:mysql://database/sakila --username=root --password=secret --query="select * from cp_rental_append limit 5" -> PROCESSO DE VER O CAMPO (ID) QUE FAZ O AUTO INCREMENTO
sqoop eval --connect jdbc:mysql://database/sakila --username=root --password=secret --query="select * from cp_rental_append order by rental_id desc limit 5" -> PEGAR O ÚLTIMO REGISTRO
sqoop import --table cp_rental_id --connect jdbc:mysql://database/sakila --username=root --password=secret --warehouse-dir /user/hive/warehouse/db_test3 -m 1 --incremental append --check-column rental_id --last-value 16049

7. Atualizar a tabela cp_rental_date no HDFS de acordo com o último registro de rental_date, atualizando os registros a partir desta data.
sqoop import --table cp_rental_date --connect jdbc:mysql://database/sakila --username=root --password=secret --warehouse-dir /user/hive/warehouse/db_test3 -m 1 --incremental lastmodified --merge-key rental_id --check-column rental_date --last-value '2005-08-23 22:50:12.0'


APPEND-> ele cria arquivos históricos no HDFS, exemplo:
Tem 1 arquivo com 10G e quer adicionar outro com 5G o 2° arquivo terá 15G ele irá aumentando
